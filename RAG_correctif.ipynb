{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf\n",
        "!pip install faiss-cpu\n",
        "!pip install langchain\n",
        "!pip install langgraph\n",
        "!pip install langchain-core\n",
        "!pip install langchain-community\n",
        "!pip install langchain-openai\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UnKgLBqSN6BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import des clés api OpenAI et Tavily\n",
        "from google.colab import userdata\n",
        "import os\n",
        "openai_api_key = userdata.get('openai_api_key')\n",
        "tavily_api_key = userdata.get('tavily_api_key')\n",
        "\n",
        "\n",
        "if openai_api_key :\n",
        "  os.environ['OPENAI_API_KEY'] = openai_api_key\n",
        "  print(\"Cle openAi disponible\")\n",
        "\n",
        "else :\n",
        "  print(\"Cle openAi non disponible veuillez la configurer dans votre environnement virtuel ou dans colab secrets\")\n",
        "  os.environ['OPENAI_API_KEY'] =\"\"\n",
        "\n",
        "if tavily_api_key :\n",
        "  os.environ['TAVILY_API_KEY'] = tavily_api_key\n",
        "  print(\"Cle Tavily disponible\")\n",
        "else :\n",
        "  print(\"Cle Tavily non disponible veuillez la configurer dans votre environnement virtuel ou dans colab secrets\")\n",
        "  os.environ['TAVILY_API_KEY'] =\"\"\n"
      ],
      "metadata": {
        "id": "Br_bes7CPjFu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Import pour le  chargement des données le decoupage et les embeddings\n",
        "\n",
        "#Diviser les grands documents en chunks gérables\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "\n",
        "#Charger les documents directement depuis des pasges web\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "#Modèle d'embeddings OpenAI pour la conversion texte --> vecteur\n",
        "\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings"
      ],
      "metadata": {
        "id": "h3B2s8HnGa_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Construire l'index à partir de PDFs\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "# Etape 1: definir les embeddings\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "\n",
        "# Etape 2: charger les documents pdf au lieu D'URl\n",
        "fichiers_pdf=[\n",
        "\n",
        "]\n",
        "doc = []\n",
        "for fichier in fichiers_pdf:\n",
        "  loader = PyPDFLoader(fichier)\n",
        "  doc.extend(loader.load()) #pypdf retourne deja une liste de documents\n",
        "\n",
        "\n",
        "print(f'{len(doc)} documents chargés depuis {len(fichiers_pdf)} fichiers pdf')\n",
        "\n",
        "\n",
        "# Etape 3: Decoupage en chunk\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size =1200 #12000 caractères maximum par chunk\n",
        "    chunk_overlap = 100 #pour eviter de couper des phrases importantes,on garde un chevauchement de 100 carateres entre les chunks\n",
        ")\n",
        "\n",
        "docs_split = text_splitter.split_documents(doc)\n",
        "print(f'{len(docs_split)} chunks créés')\n",
        "\n",
        "\n",
        "\n",
        "#Etape 4: Construction de la base de données vectorielle avec (FAISS)\n",
        "\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents=docs_split\n",
        "    embedding=embeddings\n",
        "    )\n",
        "\n",
        "\n",
        "#Etape 5: Création du retriever\n",
        "retriever = vectorstore.as_retriever()\n",
        "\n",
        "print(\"Retriever prêt\")"
      ],
      "metadata": {
        "id": "DnhRU36XISEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluateur de Recuperation (Retrieval grader)\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import chatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "\n",
        "# Etape 1: definir le schema de sortie\n",
        "class GradeDocuments(BaseModel):\n",
        "  \"\"\"score binaire pour verifier la pertinence des documents recuperer\"\"\"\n",
        "  binary_score: str = Field(description=\"Les documents sont pertinents pour la question ? : 'yes' ou 'no'\")\n",
        "\n",
        "\n",
        "#Etape 2: Initialiser le LLM avec sortie structuree\n",
        "'''On initialise le LLM avec une sortie structurée et la température à 0 pour garantir que la réponse\n",
        "du LLM respecte à chaque fois la structure definitie initialement grace à pydantic '''\n",
        "\n",
        "\n",
        "llm= chatOpenAI(\n",
        "    model =\"gpt-4.1-nano-2025-04-14\",\n",
        "    temperature=0\n",
        ")\n",
        "structured_llm_grader =llm.with_structured_output(GradeDocuments)\n",
        "\n",
        "# Etape 3: definir le prompt d'évaluation\n",
        " system_msg=\"\"\"\n",
        " Tu es un évaluateur qui évalue la pertinence d'un document récupéré par rapport à une question posée par un utilisateur.\n",
        " Si le document contient des mots clés ou une signification sémantique liés à la question, note le comme pertinent.\n",
        " Donne uniquement un score binaire: 'yes' or 'no'\n",
        " \"\"\"\n",
        "\n",
        " grade_prompt = ChatPromptTemplate.from_messages([\n",
        "     (\"system\", system_msg),\n",
        "     (\"human\", \"Document récupéré :\\n\\n{document}\\n\\nQuestion utilisateur :\\n\\n{question}\")\n",
        " ])\n",
        "\n",
        "\n",
        "\n",
        "# Etape 4: Construire la chaine d'évaluation\n",
        "retrieval_grader= grade_prompt | structured_llm_grader\n",
        "\n",
        "\n",
        "#Etape 5: tester l'évaluateur\n",
        "question=\"QU'est ce que le RAG\"\n",
        "docs=retriever.invoke(question)\n",
        "\n",
        "\n",
        "# On selectionne un chunk récupéré pour l'évaluation\n",
        "doc_txt= doc[1].page_content\n",
        "\n",
        "\n",
        "# exécuter l'évaluateur\n",
        "result=retrieval_grader.invoke({\"document\": doc_txt, \"question\": question})\n",
        "print(\"Resultat de l'évaluation:\",result)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d3wn8_cnO3O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Generer la réponse finale\n",
        "\n",
        "from langchain import hub\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain _openai import chatOpenAI\n",
        "\n",
        "\n",
        "#Étape 1: charger le template de Prompt RAG\n",
        "\n",
        "\n",
        "#rlm/rag-prompt est un prompt communautaire conçu pour les réponses de type RAG\n",
        "prompt=hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "ETape 2: Initialiser le LLM\n",
        "\n",
        "llm= chatOpenAI(\n",
        "    model =\"gpt-4.1-nano-2025-04-14\",\n",
        "    temperature=0.7   #pour que le LLM soit plus creatif\n",
        ")\n",
        "\n",
        "#Etape 3: Definir l'aide au formatage des documents\n",
        "\n",
        "format_docs(docs):\n",
        "  \"\"\"Il s'agit de joindre plusieurs documents en une seule chaine pour la variable de contexte.\"\"\"\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "\n",
        "#Etape 4: Construire la chaine RAG\n",
        "\n",
        "rag_chain=prompt | llm | StrOutputParser()\n",
        "\n",
        "\n",
        "#Etape 5:Exécuter la génération en utilisant  le contenu des documents récupérés\n",
        "\n",
        "context_text=format_docs(docs)\n",
        "\n",
        "\n",
        "generation=rag_chain.invoke({\n",
        "    \"context\": context_text,\n",
        "    \"question\": question\n",
        "})\n",
        "\n",
        "print(generation)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gUzSyioGVC5u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}